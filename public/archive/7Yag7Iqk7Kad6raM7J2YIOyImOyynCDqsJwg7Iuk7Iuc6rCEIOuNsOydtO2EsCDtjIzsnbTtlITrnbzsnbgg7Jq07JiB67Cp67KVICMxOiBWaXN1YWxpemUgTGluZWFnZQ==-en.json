{"content":"```html\n<p>Hello, this is Kang Byeong-su from the Toss Securities real-time data team.</p>\n<p>Toss Securities actively builds and utilizes real-time data pipelines. In the data domain, we configure and leverage real-time data pipelines to ingest and utilize big data in real-time. In the service domain, we utilize real-time data pipelines during the process of applying the CQRS (Command and Query Responsibility Segregation) architecture. As the service continues to grow and use cases accumulate, we are now operating thousands of real-time data pipelines. In this article, we will share our experience in building and operating real-time data pipelines on a large scale at a securities firm.</p>\n<h2>What is a Real-time Data Pipeline?</h2>\n<p>Data pipelines are a familiar concept to us. For example, we transform raw table A into table B by adding a 'category' value to make it more usable for many people. Then, those who need the quantity aggregation by 'category' use table B to create aggregation results and make them table C for use. This process involves storing intermediate outputs and transforming data to increase its usability. A real-time data pipeline is a data pipeline that performs these processes in 'real-time' as soon as an event occurs.</p>\n<p>The demand for real-time data pipelines increases as services grow. When data becomes too large due to service growth, the method of transferring a day's worth of data from an OLTP database to an OLAP database via batch dumps hits its limits. This is because the data transfer time can be too long to meet service requirements, or the sheer volume of data to be transferred puts a significant burden on the source database. As these problems worsen, the approach shifts from transferring data in batches to transferring data in real-time as it is generated.</p>\n<p>Similar situations arise in the service domain as services grow. OLTP databases, used as source databases, struggle to support aggregate queries, queries requiring large joins across multiple tables, or queries that scan entire tables as batch operations. We begin to solve this problem by applying the CQRS pattern. Ultimately, the real-time data pipeline plays the role of moving data in real-time from the OLTP database to the OLAP database.</p>\n<p>Real-time data pipelines are necessary not only for the quantitative aspect of data but also because users demand more mature services with faster responses. When displaying stock rankings or providing market news, users require content reflecting real-time market conditions, which is also achieved through real-time data pipelines.</p>\n<p>In the process of solving these problems, Toss Securities has ended up building and operating thousands of real-time data pipelines.</p>\n<h2>What Constitutes Good Operations?</h2>\n<p>The word 'real-time' makes any task incredibly demanding. When 'real-time' is added to a data pipeline, it takes on the following characteristics:</p>\n<ul>\n    <li>We must ensure minimum latency in data movement and operate 365 days a year without interruption.</li>\n    <li>Data loss must not occur during the data movement process, and duplication must be minimized.</li>\n    <li>All pipelines must be monitored to ensure all these requirements are met.</li>\n</ul>\n<p>Furthermore, each pipeline must be allocated resources independently. This is because a pipeline handling critical order executions must not be affected by the addition of other pipelines. To allocate resources independently, cluster design considering scalability is necessary to meet requirements even when hundreds or thousands of real-time data pipelines are added.</p>\n<p>From an operational perspective, pipeline visualization is also an important topic. Pipeline visualization significantly reduces operational convenience and inter-team communication costs. It's an arduous but repetitive task in operations to find out if a specific job in thousands of pipelines is currently in use, where it's being used, and what it's connected to downstream.</p>\n<p>Moreover, operational communication between different teams incurs costs. At Toss Securities, our real-time data team develops most of the real-time data pipelines and then provides the final tables. We need to explain how the data is processed. We also need to explain what data was used and where it is moving, every time. While a centralized approach to pipeline development and operation has many advantages, it is practically difficult to solve the problem of well-documented information when a small team has to handle many diverse tasks. Visualizing data pipelines to improve operational convenience and reduce inter-team communication costs is also necessary for good operations.</p>\n<p>Summarizing this, large-scale real-time data pipeline operations can be considered 'good' if the following three aspects are met:</p>\n<ol>\n    <li>SLA Compliance (Minimized Latency, Uninterrupted Operation, Minimized Loss/Duplication) and SLA Compliance Monitoring</li>\n    <li>Cluster Design Considering Scalability</li>\n    <li>Pipeline Visualization</li>\n</ol>\n<p>In this first part of the article, we will cover how Toss Securities operates by focusing on the theme of pipeline visualization.</p>\n<h2>Lineage Visualization</h2>\n<p>To operate large-scale real-time data pipelines well, pipeline visualization is essential. This is because it provides operational convenience and reduces internal communication costs.</p>\n<p>So, how should pipeline visualization be done? The jobs that constitute a data pipeline have an order and relationships with each other. In many cases, the flow is structured as follows:</p>\n<p>Raw MySQL Table A CDC issuance [A] -> Consume topic A and process data into the desired format using Flink [B] -> Consume topic B for analysis and load into Iceberg [C]</p>\n<p>Consume topic B generated by pipeline 1 and utilize it in the real-time ranking service [D]</p>\n<p>Since B is derived from A, and C is derived from B, we decided to call this 'Data Lineage'.</p>\n<p>We determined that the most suitable data structure for representing lineage containing data flow is a Graph, and specifically a DAG (Directed Acyclic Graph). In practice, DAGs are used for visualization in similar domains.</p>\n<p>We decided to create a web service to visualize all real-time data pipelines used at Toss Securities by mapping them into DAGs and allowing detailed exploration through graph traversal.</p>\n<p><strong>Figure 1. Visualization of a massive real-time pipeline constituting Toss Securities' ranking service</strong></p>\n<p>To achieve this, we first created DAG-shaped metadata tables.</p>\n<p>We stored all pipeline information in MongoDB in the format above and configured it to draw exploration results using MongoDB's Graph Search queries. MongoDB graphSearch is available from v5.1 onwards. [Link]</p>\n<p>Once all prerequisites are met, the next step is to generate metadata and draw the lineage with search results. To generate metadata for all pipelines, we first needed to organize the components that constitute the real-time data pipelines used at Toss Securities.</p>\n<ul>\n    <li><strong>Source Data Producers</strong>\n        <ul>\n            <li>Service Server</li>\n            <li>Kafka Producer</li>\n            <li>File Collection Daemons (Filebeat, fluentbit, vector)</li>\n            <li>MySQL, Oracle, MongoDB, Vitess CDC</li>\n        </ul>\n    </li>\n    <li><strong>Real-time Data Processing Engines</strong>\n        <ul>\n            <li>Flink</li>\n            <li>Kafka Connect</li>\n            <li>ClickHouse MView</li>\n            <li>Kafka Consumer</li>\n            <li>ksqlDB</li>\n        </ul>\n    </li>\n    <li><strong>Data Storage</strong>\n        <ul>\n            <li>Kafka</li>\n            <li>ClickHouse</li>\n            <li>Hadoop (Parquet, Iceberg)</li>\n            <li>Kudu</li>\n            <li>ElasticSearch</li>\n            <li>Vitess</li>\n        </ul>\n    </li>\n</ul>\n<p>We generated metadata for these systems, stored it in MongoDB, and began providing the lineage visualization web service.</p>\n<h2>Interpreting Data Pipelines with Actual Examples</h2>\n<p>We will now use the lineage visualization service to interpret pipelines configured at Toss Securities.</p>\n<p>First, search for the pipeline you are curious about. Autocompletion is supported for easy searching.</p>\n<p><strong>Figure 2. Easily search lineage with autocomplete</strong></p>\n<p>When you select the Job you want to investigate, the lineage will be drawn.</p>\n<p><strong>Figure 3. Events issued by the service are fan-out loaded into multiple storage systems</strong></p>\n<p>When we searched for market data, the following lineage was drawn. The Spring service server sends market data to the service Kafka. The market data topic is mirrored to the information system Kafka. Subsequently, information system Kafka loads the data into Hadoop, ClickHouse, and Kudu for different purposes. The stored tables are used for analysis or by services that need to serve heavy query results like aggregations.</p>\n<p>This time, let's search for Toss Securities community service tables.</p>\n<p><strong>Figure 4. MySQL tables are issued via CDC and loaded into ClickHouse service tables</strong></p>\n<p>This time, the source data issuers are two MySQL table CDCs. Database CDC publishes to Kafka topics in real-time. The Kafka topics are then loaded into ClickHouse tables. The two ClickHouse tables are triggered in real-time by ClickHouse MView upon data loading, allowing joins and processing to load the results into the final tables.</p>\n<p>The resulting final ClickHouse tables serve heavy aggregation results to the Toss Securities community lounge service, handling its traffic.</p>\n<p>Finally, we searched for Kafka topics used for inter-service server communication in MSA (Micro Service Architecture).</p>\n<p><strong>Figure 5. Kafka events issued by the service server are bidirectionally mirrored, and simultaneously consumed by the service consumer</strong></p>\n<p>The service server issues events to Kafka. The published messages are bidirectionally mirrored to Kafka in two data centers for Toss Securities' data center redundancy configuration. As a result of bidirectional mirroring, messages exist in both data centers, so the service server's Kafka Consumer consumes messages from one data center.</p>\n<p>(Note) Article related to Kafka redundancy configuration between Toss Securities DCs</p>\n<p>The lineage visualization is well done. Furthermore, each team can see how this pipeline is structured. Clicking on a curious Node or Edge in the lineage graph provides information on how it was generated.</p>\n<p><strong>Figure 6. Detailed screen showing how ClickHouse MView is processing</strong></p>\n<p>This information is stored in the Properties field in JSON format in the metadata stored in MongoDB and is utilized.</p>\n<h2>Future Outlook</h2>\n<p>We have completed showing the lineage visualization and the structure of each job for all real-time data pipelines. Moving forward, we aim to evolve this service to enhance the visibility of Toss Securities' entire system.</p>\n<p>As the first step towards that goal, we intend to link metrics for each segment.</p>\n<p>We plan to express metrics such as latency and transmission rate per second for the A -> B flow on the lineage graph. Since Toss Securities integrates and stores system metrics in ClickHouse, we will be able to quickly develop and deploy the functionality to display additional metrics.</p>\n<p>Next, we aim to become a metadata management service that reduces inter-team communication costs.</p>\n<p>Each pipeline has target SLAs, and it is necessary to express whether the current target SLA is being met. It is also important to specify the person in charge and who will be affected if a problem occurs with this pipeline. In the data processing domain, the web needs to contain detailed logic that everyone can understand. In the data loading domain, it should be a web service that provides metadata information about the pipeline, such as whether it is an append or upsert operation, and whether it is real-time or near-real-time loading.</p>\n<p>Finally, we plan to add integration with DBT.</p>\n<p>The lineage of real-time data pipelines covers the process of real-time ingestion up to the data lake. After that, it transitions into the Data Warehouse ecosystem. Currently, batch pipelines are centralized with DBT, so if we connect the real-time data pipelines and batch data pipelines well, a true end-to-end lineage will be created.</p>\n<h2>Conclusion</h2>\n<p>We have opened a service internally by visualizing the lineage of real-time data pipelines. As initially expected, it has brought significant efficiency to operating thousands of pipelines, and by allowing anyone to search and view pipelines, communication costs have been greatly reduced.</p>\n<p>In this first part, we shared the case of developing a lineage visualization service to operate large-scale real-time data pipelines effectively. There are many more important tasks for successful real-time data pipeline operations.</p>\n<ul>\n    <li>SLA Compliance (Minimized Latency, Uninterrupted Operation, Minimized Loss/Duplication) and SLA Compliance Monitoring</li>\n    <li>Cluster Design Considering Scalability</li>\n</ul>\n<p>We plan to introduce the remaining two topics in parts 2 and 3 of this article. Thank you for reading this long post.</p>\n```","createdAt":"2025-08-12T09:44:17.950Z","language":"en"}