{"title":"How Perplexity Built an AI Google","content":"<body>\n  <p>본문은 Perplexity AI가 어떻게 검색 엔진을 '답변 엔진'으로 변화시켰는지, 그리고 그 기술적 아키텍처를 상세히 설명합니다.</p>\n  <p><strong>주요 요약:</strong></p>\n  <ul>\n    <li>\n      <p>Perplexity AI는 단순한 링크 목록이 아닌, 즉각적인 답변을 제공하는 '답변 엔진'을 목표로 합니다. 이는 실시간 웹 검색과 AI 기반 요약을 통해 최신 정보를 기반으로 답변을 생성하며, '환각' 현상을 줄이고 출처를 명확히 제시하여 신뢰도를 높입니다.</p>\n    </li>\n    <li>\n      <p>핵심 기술은 Retrieval-Augmented Generation (RAG) 파이프라인으로, 5단계로 구성됩니다:</p>\n      <ul>\n        <li>쿼리 의도 파싱: LLM을 사용하여 사용자의 의도를 깊이 이해합니다.</li>\n        <li>실시간 웹 검색: 최신 관련 정보를 웹에서 검색합니다.</li>\n        <li>스니펫 추출 및 맥락화: 관련성 높은 텍스트 조각을 추출하여 LLM에 전달할 맥락을 만듭니다.</li>\n        <li>인용 포함 답변 생성: 추출된 맥락을 기반으로 AI가 답변을 생성하고, 모든 정보에 출처를 명시합니다.</li>\n        <li>대화형 개선: 이전 대화 내용을 기억하며 후속 질문에 대한 답변을 개선합니다.</li>\n      </ul>\n    </li>\n    <li>\n      <p>Perplexity의 강점은 특정 LLM 개발이 아닌, 다양한 LLM과 고성능 검색 시스템을 **오케스트레이션**하는 능력에 있습니다. 모델에 구애받지 않는(model-agnostic) 아키텍처를 통해, 자체 개발한 'Sonar' 모델과 OpenAI, Anthropic 등 외부의 최신 모델을 효율적으로 라우팅하여 사용합니다. 이는 비용과 성능을 최적화하는 핵심 전략입니다.</p>\n    </li>\n    <li>\n      <p><strong>검색 엔진(Retrieval Engine)</strong>은 Vespa AI를 기반으로 구축되었습니다. Vespa는 벡터 검색, 렉시컬 검색, 구조화된 필터링, 머신러닝 랭킹 등 다양한 검색 기술을 통합하여 대규모의 실시간 RAG를 지원합니다. Perplexity는 복잡한 검색 인덱싱 대신 Vespa를 활용하여 RAG 오케스트레이션, Sonar 모델 파인튜닝, ROSE 추론 엔진 최적화에 집중합니다.</p>\n      <p><strong>주요 특징:</strong></p>\n      <ul>\n        <li>웹 스케일 인덱싱: 2000억 개 이상의 URL을 추적하는 방대한 인프라를 사용합니다.</li>\n        <li>실시간 신선도: 초당 수만 건의 인덱스 업데이트를 처리하여 최신 정보를 제공합니다.</li>\n        <li>세분화된 콘텐츠 이해: 문서를 '청크' 단위로 분할하여 관련성 높은 부분만 추출합니다.</li>\n        <li>AI 기반 콘텐츠 파싱: 자체 학습하는 AI 모듈로 다양한 웹사이트의 콘텐츠를 효과적으로 추출합니다.</li>\n        <li>하이브리드 검색 및 랭킹: 벡터 검색(의미 기반)과 렉시컬 검색(정확도 기반)을 결합하고, 머신러닝으로 최종 순위를 결정합니다.</li>\n      </ul>\n    </li>\n    <li>\n      <p><strong>생성 엔진(Generation Engine)</strong>은 Perplexity 자체 개발 모델인 'Sonar'와 외부 최신 모델(GPT, Claude 등)을 조합하여 사용합니다. Sonar 모델은 오픈 소스 모델을 기반으로 요약, 인용, 사실 기반 답변 등의 특화된 기능을 학습시켰습니다. Amazon Bedrock을 통해 다양한 외부 모델을 통합합니다.</p>\n    </li>\n    <li>\n      <p><strong>추론 스택(Inference Stack)</strong>은 AI 모델을 빠르고 저렴하게 실행하기 위한 핵심 시스템으로, 자체 개발한 'ROSE' 엔진을 사용합니다. ROSE는 유연성과 최적화에 중점을 두며, Python과 Rust를 사용하고 NVIDIA H100 GPU와 Kubernetes를 활용하여 AWS 클라우드 상에서 대규모로 운영됩니다. 이는 자체 스택 구축을 통해 응답 속도와 비용 효율성을 극대화하는 전략입니다.</p>\n    </li>\n  </ul>\n  <p>결론적으로, Perplexity AI의 성공은 특정 LLM이 아닌, Vespa 기반 검색 엔진, 유연한 오케스트레이션 레이어, 고도로 최적화된 자체 추론 스택 등 각 구성 요소가 유기적으로 결합된 엔드투엔드 시스템 엔지니어링의 결과입니다.</p>\n</body>\n","createdAt":"2025-11-03T16:31:09.000+00:00","link":"https://blog.bytebytego.com/p/how-perplexity-built-an-ai-google","language":"ko"}