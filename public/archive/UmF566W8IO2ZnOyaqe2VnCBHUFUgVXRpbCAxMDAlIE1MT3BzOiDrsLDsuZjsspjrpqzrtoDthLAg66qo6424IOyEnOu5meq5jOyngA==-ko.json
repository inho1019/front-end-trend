{"content":"<p>네이버 사내 기술 교류 행사인 NAVER ENGINEERING DAY 2025에서 발표된 AI/ML 분산 처리 프레임워크 Ray 활용 세션 내용을 공개합니다.</p>\n\n<h2>발표 내용</h2>\n<p>AI/ML 분산 처리 프레임워크인 Ray를 활용하여 GPU Util 100%를 달성한 배치처리 기법과 확장 가능한 모델 서빙 아키텍처를 소개합니다.</p>\n\n<h2>발표 대상</h2>\n<ul>\n    <li>배치 파이프라인 설계와 모델 서빙 자동화를 담당하는 분</li>\n    <li>Ray 기반 인프라 운영 및 GPU 클러스터 관리 업무를 수행하는 분</li>\n    <li>Ray Serve를 활용해 고성능 모델 서빙 API를 설계·배포·운영하는 분</li>\n    <li>Ray LLM(vLLM) 기반 LLM 추론 파이프라인을 구성·확장하고, 내부 모델 레지스트리를 연동하는 분</li>\n</ul>\n\n<h2>목차</h2>\n<ol>\n    <li>\n        <h3>Introduction to Ray</h3>\n        <p>Ray에 대한 소개 및 Core Architecture에 대한 이해</p>\n    </li>\n    <li>\n        <h3>Ray Data: GPU Util 100% Bach Inference를 위한 수난기</h3>\n        <ul>\n            <li>기존 구조와 도입된 구조 비교</li>\n            <li>TroubleShooting 4건</li>\n            <li>PipelineStep 추상 클래스 소개</li>\n        </ul>\n    </li>\n    <li>\n        <h3>Ray Serve: 배치 + 서빙, 두 마리 토끼를 잡다</h3>\n        <ul>\n            <li>Offline Serving UseCase</li>\n            <li>GPU 자원 효율성 실험</li>\n            <li>ModelInference, BaseDeployment 인터페이스 소개</li>\n        </ul>\n    </li>\n    <li>\n        <h3>Ray LLM: ServeManager를 활용한 LLM 배포 (with vLLM)</h3>\n        <ul>\n            <li>ServeManager 구조 소개</li>\n            <li>TroubleShooting 4건</li>\n        </ul>\n    </li>\n    <li><h3>Conclusion</h3></li>\n</ol>\n\n<h2>NAVER ENGINEERING DAY란?</h2>\n<p>NAVER에서는 사내 개발 경험과 기술 트렌드를 교류할 수 있는 프로그램이 많이 있습니다. 그중 매회 평균 70개 이상의 발표가 이루어지는 NAVER ENGINEERING DAY를 빼놓을 수 없습니다. 2016년부터 시작된 ENGINEERING DAY는 실무에서의 기술 개발 경험과 새로운 기술과 플랫폼 도입 시 유용하게 활용될 수 있는 팁 등을 공유하며 서로 배우고 성장하는 네이버의 대표적인 사내 개발자 행사입니다. 올해 진행된 NAVER ENGINEERING DAY의 일부 세션을 공개합니다.</p>","createdAt":"2025-08-12T10:03:55.987Z","language":"ko"}